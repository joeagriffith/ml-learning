{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777174ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\anaconda3\\envs\\ml-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecNormalize, VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from rlgym.envs import Match\n",
    "from rlgym.utils.action_parsers import DiscreteAction\n",
    "from rlgym.utils.obs_builders import AdvancedObs\n",
    "from rlgym.utils.state_setters import DefaultState, RandomState\n",
    "from rlgym.utils.terminal_conditions.common_conditions import TimeoutCondition, NoTouchTimeoutCondition, GoalScoredCondition\n",
    "from rlgym.utils.reward_functions.common_rewards.misc_rewards import EventReward, ConstantReward, VelocityReward, SaveBoostReward\n",
    "from rlgym.utils.reward_functions.common_rewards.player_ball_rewards import VelocityPlayerToBallReward\n",
    "from rlgym.utils.reward_functions.common_rewards.ball_goal_rewards import VelocityBallToGoalReward\n",
    "from rlgym.utils.reward_functions.common_rewards.conditional_rewards import RewardIfBehindBall\n",
    "from rlgym.utils.reward_functions import CombinedReward\n",
    "from rlgym_tools.sb3_utils import SB3MultipleInstanceEnv\n",
    "\n",
    "from egocentric_obs import EgocentricObs\n",
    "from rewards import RewardIfGoalside, RewardIfShouldShadow1s, PossessionReward, RewardIfPlayerBallY, PlayerBallYDistReward, TimestepReward, MultiplyRewards, RewardIfGrounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dfff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 8                                             # Number of ticks to repeat an action\n",
    "half_life_seconds = 5                                      # Easier to conceptualize, after this many seconds the reward discount is 0.5\n",
    "\n",
    "fps = 240 / frame_skip\n",
    "gamma = np.exp(np.log(0.5) / (fps * half_life_seconds))    # Quick mafs\n",
    "agents_per_match = 2\n",
    "num_instances = 1 #18\n",
    "target_steps = 1_000_000\n",
    "steps = target_steps // (num_instances * agents_per_match) #making sure the experience counts line up properly\n",
    "batch_size = target_steps//10                              #getting the batch size down to something more manageable - 100k in this case\n",
    "training_interval = 25_000_000\n",
    "mmr_save_frequency = 50_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0de3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'test'#'Egor_512'\n",
    "def exit_save(model):\n",
    "    model.save(f'models/{model_name}/exit_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb90ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match():  # Need to use a function so that each instance can call it and produce their own objects\n",
    "    return Match(\n",
    "        team_size=1,\n",
    "        tick_skip=frame_skip,\n",
    "        reward_function=CombinedReward(\n",
    "        (\n",
    "#             RewardIfShouldShadow1s(ConstantReward()),\n",
    "#             RewardIfGoalside(ConstantReward()),\n",
    "            SaveBoostReward(),\n",
    "#             RewardIfPlayerBallY(PlayerBallYDistReward()),\n",
    "            RewardIfGrounded(ConstantReward()),\n",
    "            VelocityReward(), \n",
    "#             VelocityPlayerToBallReward(),\n",
    "#             VelocityBallToGoalReward(),\n",
    "#             EventReward(\n",
    "#                 team_goal=100.0,\n",
    "#                 concede=-100.0,\n",
    "#                 shot=5.0,\n",
    "#                 save=30.0,\n",
    "#                 demo=10.0,\n",
    "#             ),\n",
    "        ),\n",
    "        (0.05, 0.01, 0.05)),\n",
    "        # self_play=True,  in rlgym 1.2 'self_play' is depreciated. Uncomment line if using an earlier version and comment out spawn_opponents\n",
    "        spawn_opponents=True,\n",
    "#         terminal_conditions=[TimeoutCondition(fps * 100), NoTouchTimeoutCondition(fps * 20), GoalScoredCondition()],\n",
    "        terminal_conditions=[TimeoutCondition(fps * 10)],\n",
    "        obs_builder=EgocentricObs(),  # Not that advanced, good default\n",
    "        state_setter=RandomState(),  # Resets to kickoff position\n",
    "        action_parser=DiscreteAction(n_bins=9)  # Discrete > Continuous don't @ me\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5d29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\anaconda3\\envs\\ml-env\\lib\\site-packages\\rlgym\\utils\\math.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.divide(vec, vecmag(vec))\n"
     ]
    }
   ],
   "source": [
    "env = SB3MultipleInstanceEnv(get_match, num_instances, wait_time=45)# Start 1 instances, waiting 60 seconds between each\n",
    "env = VecCheckNan(env)                                # Optional\n",
    "env = VecMonitor(env)                                 # Recommended, logs mean reward and ep_len to Tensorboard\n",
    "env = VecNormalize(env, norm_obs=False, gamma=gamma)  # Highly recommended, normalizes rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58323e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ({model_id}) not found, creating new model.\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model_id = 'exit_save'\n",
    "try:\n",
    "    model = PPO.load(\n",
    "        f'models/{model_name}/{model_id}.zip',\n",
    "        env,\n",
    "        device=\"cuda\",\n",
    "        custom_objects={\"n_envs\": env.num_envs}, #automatically adjusts to users changing instance count, may encounter shaping error otherwise\n",
    "        # If you need to adjust parameters mid training, you can use the below example as a guide\n",
    "        #custom_objects={\"n_envs\": env.num_envs, \"n_steps\": steps, \"batch_size\": batch_size, \"n_epochs\": 10, \"learning_rate\": 5e-5}\n",
    "    )\n",
    "    print(f\"Loaded: {model_id}.\")\n",
    "except:\n",
    "    print(\"model ({model_id}) not found, creating new model.\")\n",
    "    from torch.nn import ELU\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=ELU,\n",
    "        net_arch=[256, 256, 256, dict(pi=[512, 512, 512], vf=[512, 512, 512])],\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        n_epochs=10,                                                             # PPO calls for multiple epochs\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        learning_rate=5e-5,                                                      # Around this is fairly common for PPO Originally 5e-5\n",
    "        ent_coef=0.01,                                                           # From PPO Atari\n",
    "        vf_coef=1.,                                                              # From PPO Atari\n",
    "        gamma=gamma,                                                             # Gamma as calculated using half-life\n",
    "        verbose=3,                                                               # Print out all the info as we're going\n",
    "        batch_size=batch_size,                                                   # Batch size as high as possible within reason\n",
    "        n_steps=steps,                                                           # Number of steps to perform before optimizing network\n",
    "        tensorboard_log=f'logs/{model_name}',                                    # `tensorboard --logdir out/logs` in terminal to see graphs\n",
    "        device=\"cuda\"                                                            # Uses GPU if available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a905ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/test\\PPO_0\n"
     ]
    }
   ],
   "source": [
    "# Save model every so often\n",
    "# Divide by num_envs (number of agents) because callback only increments every time all agents have taken a step\n",
    "# This saves to specified folder with a specified name\n",
    "callback = CheckpointCallback(round(5_000_000 / env.num_envs), save_path=f\"models/{model_name}\", name_prefix=model_name)\n",
    "\n",
    "try:\n",
    "    mmr_model_target_count = model.num_timesteps + mmr_save_frequency\n",
    "    while True:\n",
    "        #may need to reset timesteps when you're running a different number of instances than when you saved the model\n",
    "        model.learn(training_interval, callback=callback, reset_num_timesteps=False) #can ignore callback if training_interval < callback target\n",
    "        model.save(f\"models/{model_name}/exit_save\")\n",
    "        if model.num_timesteps >= mmr_model_target_count:\n",
    "            model.save(f\"mmr_models/{model_name}/{model.num_timesteps}\")\n",
    "            mmr_model_target_count += mmr_save_frequency\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting training\")\n",
    "\n",
    "print(\"Saving model\")\n",
    "exit_save(model)\n",
    "print(\"Save complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9504b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42024388 0.52496365 0.57216038]\n",
      " [0.95141792 0.56389961 0.29002981]\n",
      " [0.67104232 0.4345674  0.56028025]\n",
      " [0.52341958 0.09264067 0.3512735 ]\n",
      " [0.83544003 0.86887197 0.82118781]]\n",
      "[[-0.57975612 -0.47503635 -0.42783962]\n",
      " [-0.04858208 -0.43610039 -0.70997019]\n",
      " [-0.32895768 -0.5654326  -0.43971975]\n",
      " [-0.47658042 -0.90735933 -0.6487265 ]\n",
      " [-0.16455997 -0.13112803 -0.17881219]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(5, 3)\n",
    "y = np.array([1, 1, 1])\n",
    "print(x)\n",
    "print(x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_save(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06097eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1267e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
