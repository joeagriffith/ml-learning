{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.HAugPC.train import train as train_haugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.HAugPC.model import HAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    # transforms.RandomCrop(20),\n",
    "    # transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = HAugPC\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-3'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{model_name}.pth'\n",
    "# log_dir = None\n",
    "# save_dir = None\n",
    "model = Model([784, 512, 256, 128], 5, 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     18\u001b[0m     writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# train_augpc(\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     save_every=5,\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtrain_haugpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_on_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# train_byol(\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#     save_every=5,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\HAugPC\\train.py:104\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, val_dataset, num_epochs, batch_size, lr, wd, aug_scaler, learn_on_ss, normalise, writer, save_dir, save_every)\u001b[0m\n\u001b[0;32m    102\u001b[0m     images_aug \u001b[38;5;241m=\u001b[39m F_v2\u001b[38;5;241m.\u001b[39maffine(images, angle\u001b[38;5;241m=\u001b[39mangle, translate\u001b[38;5;241m=\u001b[39m(translate_x, translate_y), scale\u001b[38;5;241m=\u001b[39mscale, shear\u001b[38;5;241m=\u001b[39mshear)\n\u001b[0;32m    103\u001b[0m     targets \u001b[38;5;241m=\u001b[39m model(images_aug\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m), return_all_latents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 104\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalise:\n\u001b[0;32m    106\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [F\u001b[38;5;241m.\u001b[39mnormalize(t, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\HAugPC\\model.py:80\u001b[0m, in \u001b[0;36mHAugPC.predict\u001b[1;34m(self, x, a)\u001b[0m\n\u001b[0;32m     78\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 80\u001b[0m     pred, x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\HAugPC\\model.py:41\u001b[0m, in \u001b[0;36mLayer.predict\u001b[1;34m(self, x, a)\u001b[0m\n\u001b[0;32m     39\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m     40\u001b[0m z_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition(torch\u001b[38;5;241m.\u001b[39mcat([z, a], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 41\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred, z\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    # train_augpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=128,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    #     aug_scaler='none'\n",
    "    # )\n",
    "\n",
    "    # train_laugpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=0.0001,\n",
    "    #     wd=1.5e-6,\n",
    "    #     beta=0.05,\n",
    "    #     tau_0=0.925,\n",
    "    #     tau_e=0.95,\n",
    "    #     tau_T=100,\n",
    "    #     aug_scaler='none',\n",
    "    #     learn_on_ss=False,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    train_haugpc(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        num_epochs=500,\n",
    "        batch_size=256,\n",
    "        lr=0.0001,\n",
    "        wd=1.5e-6,\n",
    "        aug_scaler='none',\n",
    "        learn_on_ss=False,\n",
    "        normalise=False,\n",
    "        writer=writer,\n",
    "        save_dir=save_dir,\n",
    "        save_every=5,\n",
    "    )\n",
    "\n",
    "\n",
    "    # train_byol(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=0.001,\n",
    "    #     wd=1.5e-6,\n",
    "    #     augmentation=augmentation,\n",
    "    #     beta=None,\n",
    "    #     tau_0=0.996,\n",
    "    #     tau_e=0.999,\n",
    "    #     tau_T=100,\n",
    "    #     normalise=True,\n",
    "    #     learn_on_ss=False,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    # train_simclr(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=0.0,\n",
    "    #     temperature=1.0,\n",
    "    #     augmentation=augmentation,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1400x28 and 784x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# collect 100 of each target index from train_set.targets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmnist_linear_1k_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Examples\\MNIST\\mnist_linear_1k.py:65\u001b[0m, in \u001b[0;36mmnist_linear_1k_eval\u001b[1;34m(model, writer)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 65\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m classifier(z)\n\u001b[0;32m     67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_pred, y)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\HAugPC\\model.py:67\u001b[0m, in \u001b[0;36mHAugPC.forward\u001b[1;34m(self, x, return_all_latents)\u001b[0m\n\u001b[0;32m     65\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 67\u001b[0m     xs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_all_latents:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\HAugPC\\model.py:35\u001b[0m, in \u001b[0;36mLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 35\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1400x28 and 784x512)"
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f950b8bf675c458bb9a3d5687495816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    img_pred = model.predict(img, action)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
