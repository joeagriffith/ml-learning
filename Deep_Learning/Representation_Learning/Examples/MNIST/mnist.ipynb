{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LAugPC\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-beta0.925_loBS'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/out/mnist/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/out/mnist/models/{model_name}.pth'\n",
    "# log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(in_features=1, num_actions=5, backbone=backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 33\u001b[0m\n\u001b[0;32m     18\u001b[0m     writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# train_augpc(\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     aug_scaler='none'\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrain_laugpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.925\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# train_byol(\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#     save_every=5,\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\LAugPC\\train.py:87\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(online_model, train_dataset, val_dataset, num_epochs, batch_size, lr, wd, beta, writer, save_dir, save_every, aug_scaler)\u001b[0m\n\u001b[0;32m     84\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(z_pred, z_target)\n\u001b[0;32m     86\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimiser)\n\u001b[0;32m     89\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    # train_augpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=128,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    #     aug_scaler='none'\n",
    "    # )\n",
    "\n",
    "    train_laugpc(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        num_epochs=500,\n",
    "        batch_size=6,\n",
    "        lr=0.001,\n",
    "        wd=1.5e-6,\n",
    "        beta=0.925,\n",
    "        writer=writer,\n",
    "        save_dir=save_dir,\n",
    "        save_every=5,\n",
    "    )\n",
    "\n",
    "\n",
    "    # train_byol(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     augmentation=augmentation,\n",
    "    #     beta=0.996,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    # train_simclr(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=0.0,\n",
    "    #     temperature=1.0,\n",
    "    #     augmentation=augmentation,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.7945999503135681\n"
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist1k_linear_eval(model, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f950b8bf675c458bb9a3d5687495816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    img_pred = model.predict(img, action)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
