{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.HAugPC.train import train as train_haugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.HAugPC.model import HAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.train import train as train_simsiam\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.model import SimSiam\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    # transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = SimSiam\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-5'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{model_name}.pth'\n",
    "# log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(1, backbone=backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 89\u001b[0m\n\u001b[0;32m     18\u001b[0m     writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# train_augpc(\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#     save_every=5,\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[43mtrain_simsiam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_on_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# train_simclr(\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m#     train_set,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#     save_every=5,\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\SimSiam\\train.py:104\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, val_dataset, num_epochs, batch_size, lr, wd, augmentation, beta, normalise, learn_on_ss, writer, save_dir, save_every)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m    103\u001b[0m     x1, x2 \u001b[38;5;241m=\u001b[39m augmentation(images), augmentation(images)\n\u001b[1;32m--> 104\u001b[0m     z1, z2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m, model(x2)\n\u001b[0;32m    105\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(z1), model\u001b[38;5;241m.\u001b[39mproject(z2)\n\u001b[0;32m    106\u001b[0m     y1, y2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(p1), model\u001b[38;5;241m.\u001b[39mpredict(p2)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\SimSiam\\model.py:46\u001b[0m, in \u001b[0;36mSimSiam.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\models\\alexnet.py:48\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    # train_augpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=128,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    #     aug_scaler='none'\n",
    "    # )\n",
    "\n",
    "    # train_laugpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=0.0001,\n",
    "    #     wd=1.5e-6,\n",
    "    #     beta=0.05,\n",
    "    #     tau_0=0.925,\n",
    "    #     tau_e=0.95,\n",
    "    #     tau_T=100,\n",
    "    #     aug_scaler='none',\n",
    "    #     learn_on_ss=False,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    # train_haugpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=0.000001,\n",
    "    #     wd=1.5e-6,\n",
    "    #     aug_scaler='none',\n",
    "    #     learn_on_ss=False,\n",
    "    #     normalise=False,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # train_byol(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=0.001,\n",
    "    #     wd=1.5e-6,\n",
    "    #     augmentation=augmentation,\n",
    "    #     beta=None,\n",
    "    #     tau_0=0.996,\n",
    "    #     tau_e=0.999,\n",
    "    #     tau_T=100,\n",
    "    #     normalise=True,\n",
    "    #     learn_on_ss=False,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    train_simsiam(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        num_epochs=500,\n",
    "        batch_size=256,\n",
    "        lr=0.001,\n",
    "        wd=0.0001,\n",
    "        augmentation=augmentation,\n",
    "        beta=None,\n",
    "        learn_on_ss=False,\n",
    "        writer=writer,\n",
    "        save_dir=save_dir,\n",
    "        save_every=5,\n",
    "    )\n",
    "\n",
    "\n",
    "    # train_simclr(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=0.0,\n",
    "    #     temperature=1.0,\n",
    "    #     augmentation=augmentation,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.7276999354362488\n"
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eca521d7b8542f28062975f463af0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    img_pred = model.predict(img, action)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
