{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548f8013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.conv import _ConvNd\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from typing import Optional, List, Tuple, Union\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eb8b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25200b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 0.5\n",
    "NU = 1.0\n",
    "ETA = 0.05\n",
    "STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e334ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCInputLayer(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        \n",
    "    def init_vars(self):\n",
    "        e = torch.zeros((self.size, 1)).to(device)\n",
    "        return e\n",
    "        \n",
    "    def step(self, x, td_pred):\n",
    "        return x - td_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a029ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(_ConvNd):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da2989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1.shape: torch.Size([16, 7, 7])\n",
      "td_pred1.shape: torch.Size([1, 28, 28])\n",
      "r2.shape: torch.Size([32, 6, 6])\n",
      "td_pred2.shape: torch.Size([16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class PCConv2d(_ConvNd):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = False,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super(PCConv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "    def _output_padding(self, input: Tensor, output_size: Optional[List[int]],\n",
    "                        stride: List[int], padding: List[int], kernel_size: List[int],\n",
    "                        num_spatial_dims: int, dilation: Optional[List[int]] = None) -> List[int]:\n",
    "        if output_size is None:\n",
    "            ret = _single(self.output_padding)  # converting to list if was not already\n",
    "        else:\n",
    "            has_batch_dim = input.dim() == num_spatial_dims + 2\n",
    "            num_non_spatial_dims = 2 if has_batch_dim else 1\n",
    "            if len(output_size) == num_non_spatial_dims + num_spatial_dims:\n",
    "                output_size = output_size[num_non_spatial_dims:]\n",
    "            if len(output_size) != num_spatial_dims:\n",
    "                raise ValueError(\n",
    "                    \"ConvTranspose{}D: for {}D input, output_size must have {} or {} elements (got {})\"\n",
    "                    .format(num_spatial_dims, input.dim(), num_spatial_dims,\n",
    "                            num_non_spatial_dims + num_spatial_dims, len(output_size)))\n",
    "\n",
    "            min_sizes = torch.jit.annotate(List[int], [])\n",
    "            max_sizes = torch.jit.annotate(List[int], [])\n",
    "            for d in range(num_spatial_dims):\n",
    "                dim_size = ((input.size(d + num_non_spatial_dims) - 1) * stride[d] -\n",
    "                            2 * padding[d] +\n",
    "                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)\n",
    "                min_sizes.append(dim_size)\n",
    "                max_sizes.append(min_sizes[d] + stride[d] - 1)\n",
    "\n",
    "            for i in range(len(output_size)):\n",
    "                size = output_size[i]\n",
    "                min_size = min_sizes[i]\n",
    "                max_size = max_sizes[i]\n",
    "                if size < min_size or size > max_size:\n",
    "                    raise ValueError((\n",
    "                        \"requested an output size of {}, but valid sizes range \"\n",
    "                        \"from {} to {} (for an input of {})\").format(\n",
    "                            output_size, min_sizes, max_sizes, input.size()[2:]))\n",
    "\n",
    "            res = torch.jit.annotate(List[int], [])\n",
    "            for d in range(num_spatial_dims):\n",
    "                res.append(output_size[d] - min_sizes[d])\n",
    "\n",
    "            ret = res\n",
    "        return ret\n",
    "      \n",
    "    def _conv_forward(self, input: Tensor, bias: Optional[Tensor]=None):\n",
    "        if self.padding_mode != 'zeros':\n",
    "                        raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')\n",
    "        return F.conv2d(input, self.weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "    def _conv_transpose(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:\n",
    "        if self.padding_mode != 'zeros':\n",
    "            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')\n",
    "        num_spatial_dims = 2\n",
    "        output_padding = self._output_padding(\n",
    "            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\n",
    "            num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n",
    "\n",
    "        return F.conv_transpose2d(\n",
    "            input, self.weight, self.bias, self.stride, self.padding,\n",
    "            output_padding, self.groups, self.dilation)    \n",
    "    \n",
    "    def init_vars(self):\n",
    "        r = torch.zeros((self.out_channels, 1)).to(device)\n",
    "        e = torch.zeros((self.out_channels, 1)).to(device)\n",
    "        return r, e\n",
    "\n",
    "    def pred(self, r: Tensor) -> Tensor:\n",
    "        return F.relu(self._conv_transpose(r.unsqueeze(0)))\n",
    "    \n",
    "    def step(self, e_inf, r, e, td_pred) -> Tensor:\n",
    "        r = NU*r + MU*self._conv_forward(e_inf.unsqueeze(0)).reshape((-1, 1)) - ETA*e\n",
    "        e = r - td_pred\n",
    "        return r, e\n",
    "    \n",
    "x = torch.rand((1,28,28)).to(device)\n",
    "layer1 = PCConv2d(1, 16, (4,4), (4,4), device=device)\n",
    "r1 = layer1._conv_forward(x)\n",
    "print(\"r1.shape:\", r1.shape)\n",
    "td_pred1 = layer1._conv_transpose(r1)\n",
    "print(\"td_pred1.shape:\", td_pred1.shape)\n",
    "e1 = torch.rand(r1.shape).to(device)\n",
    "\n",
    "layer2 = PCConv2d(16, 32, (2,2), (1,1), device=device)\n",
    "r2 = layer2._conv_forward(e1)\n",
    "print(\"r2.shape:\", r2.shape)\n",
    "td_pred2 = layer2._conv_transpose(r2)\n",
    "print(\"td_pred2.shape:\", td_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70d4aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCConv2d(_ConvNd):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super(PCConv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, padding_mode, **factory_kwargs)\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "\n",
    "    \n",
    "    def pred(self, r):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')\n",
    "\n",
    "        assert isinstance(self.padding, tuple)\n",
    "        # One cannot replace List by Tuple or Sequence in \"_output_padding\" because\n",
    "        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\n",
    "        num_spatial_dims = 2\n",
    "        output_padding = 0 # <===========HACKY\n",
    "#         output_padding = self._output_padding(\n",
    "#             input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\n",
    "#             num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n",
    "\n",
    "        return F.relu(F.conv_transpose2d(\n",
    "            r.unsqueeze(0), self.weight, self.bias, self.stride, self.padding,\n",
    "            output_padding, self.groups, self.dilation)).reshape((-1, 1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4161e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLayer(nn.Module):\n",
    "    def __init__(self, size_prev, size):\n",
    "        super().__init__()\n",
    "        self.size, self.size_prev = size, size_prev\n",
    "        \n",
    "        U = torch.zeros((size_prev, size)).to(device)\n",
    "        self.U = nn.Parameter(U)\n",
    "        nn.init.kaiming_uniform_(self.U, a=25) # <=== To Revisit\n",
    "        \n",
    "#         V = torch.zeros((size, size_prev)).to(device)\n",
    "#         self.V = nn.Parameter(V)\n",
    "#         nn.init.kaiming_uniform_(self.V, a=25) # <=== To Revisit\n",
    "        \n",
    "    def init_vars(self):\n",
    "        r = torch.zeros((self.size, 1)).to(device)\n",
    "        e = torch.zeros((self.size, 1)).to(device)\n",
    "        return r, e\n",
    "        \n",
    "    def pred(self, r):\n",
    "        return F.relu(torch.mm(self.U, r))\n",
    "\n",
    "    def step(self, e_inf, r, e, td_pred):\n",
    "        r = NU*r + MU*torch.mm(self.U.t(),e_inf) - ETA*e\n",
    "        e = r - td_pred      \n",
    "        return r, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e67bc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, h1_size, h2_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.pc0 = PCInputLayer(input_size)\n",
    "        self.pc1 = PCConv2d(input_size, h1_size, kernel_size=4, stride=4)\n",
    "        self.pc2 = PCConv2d(h1_size, h2_size, kernel_size=4, stride=4)\n",
    "        self.pc3 = PCConv2d(h2_size, num_classes, kernel_size=4, stride=4)\n",
    "    \n",
    "    def train(self, x, targets, debug=False):\n",
    "        pc0_e = self.pc0.init_vars()\n",
    "        pc1_r, pc1_e = self.pc1.init_vars()\n",
    "        pc2_r, pc2_e = self.pc2.init_vars()\n",
    "        pc3_r, pc3_e = self.pc3.init_vars()\n",
    "        \n",
    "        for _ in range(STEPS):\n",
    "            pc0_e = self.pc0.step(x, self.pc1.pred(pc1_r))\n",
    "            pc1_r, pc1_e = self.pc1.step(pc0_e, pc1_r, pc1_e, self.pc2.pred(pc2_r))\n",
    "            pc2_r, pc2_e = self.pc2.step(pc1_e, pc2_r, pc2_e, self.pc3.pred(pc3_r))\n",
    "            pc3_r, pc3_e = self.pc3.step(pc2_e, pc3_r, pc3_e, targets)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"printing pc3_r....\")\n",
    "            print(pc3_r)\n",
    "            print(\"printing pc3_e...\")\n",
    "            print(pc3_e)\n",
    "            \n",
    "        pc0_err = pc0_e.square().sum()/self.pc0.size\n",
    "        pc1_err = pc1_e.square().sum()/self.pc1.size\n",
    "        pc2_err = pc2_e.square().sum()/self.pc2.size\n",
    "        pc3_err = pc3_e.square().sum()/self.pc3.size\n",
    "            \n",
    "        total_sqr_err =  pc0_err + pc1_err + pc2_err + 10*pc3_err\n",
    "        return total_sqr_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c326583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "H1_SIZE = 784\n",
    "H2_SIZE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "524ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a676adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(INPUT_SIZE, H1_SIZE, H2_SIZE, NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abda876f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [784, 784, 4, 4], expected input[1, 1, 784, 1] to have 784 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((NUM_CLASSES, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m         targets[y[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#         print(\"targets: \", targets)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#         print(loss)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[83], line 17\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, x, targets, debug)\u001b[0m\n\u001b[0;32m     14\u001b[0m pc3_r, pc3_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc3\u001b[38;5;241m.\u001b[39minit_vars()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(STEPS):\n\u001b[1;32m---> 17\u001b[0m     pc0_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc0\u001b[38;5;241m.\u001b[39mstep(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc1_r\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     pc1_r, pc1_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc1\u001b[38;5;241m.\u001b[39mstep(pc0_e, pc1_r, pc1_e, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc2\u001b[38;5;241m.\u001b[39mpred(pc2_r))\n\u001b[0;32m     19\u001b[0m     pc2_r, pc2_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc2\u001b[38;5;241m.\u001b[39mstep(pc1_e, pc2_r, pc2_e, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc3\u001b[38;5;241m.\u001b[39mpred(pc3_r))\n",
      "Cell \u001b[1;32mIn[81], line 52\u001b[0m, in \u001b[0;36mPCConv2d.pred\u001b[1;34m(self, r)\u001b[0m\n\u001b[0;32m     47\u001b[0m         output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# <===========HACKY\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#         output_padding = self._output_padding(\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#             input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#             num_spatial_dims, self.dilation)  # type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [784, 784, 4, 4], expected input[1, 1, 784, 1] to have 784 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0000001\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mean_loss = 0\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (data, y) in enumerate(train_loader):\n",
    "        \n",
    "        x = data.reshape((-1, 1)).to(device)\n",
    "        targets = torch.zeros((NUM_CLASSES, 1)).to(device)\n",
    "        targets[y[0]] = 1\n",
    "\n",
    "        loss = model.train(x, targets, debug=False)\n",
    "#         print(\"targets: \", targets)\n",
    "#         print(loss)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        \n",
    "        mean_loss += loss\n",
    "        if batch_idx % 64 == 0:\n",
    "            print(\"mean_loss:\",mean_loss / 64)\n",
    "            mean_loss = 0\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52ae58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef14547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
