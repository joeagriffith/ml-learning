{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.train import train as train_simsiam\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.model import SimSiam\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC2.train import train as train_laugpc2\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC2.model import LAugPC2\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval\n",
    "from Deep_Learning.Representation_Learning.Utils.functional import get_optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    # transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LAugPC\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-new2'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{model_name}.pth'\n",
    "# log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(1, 5, backbone=backbone).to(device)\n",
    "# model = Model(1, backbone).to(device)\n",
    "\n",
    "\n",
    "optimiser = get_optimiser(\n",
    "    model, \n",
    "    'AdamW', \n",
    "    lr=3e-4, \n",
    "    wd=0.004, \n",
    "    exclude_bias=True, \n",
    "    exclude_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m\n\u001b[0;32m     21\u001b[0m     train_augpc(\n\u001b[0;32m     22\u001b[0m         model,\n\u001b[0;32m     23\u001b[0m         optimiser,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m         aug_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, LAugPC):\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mtrain_laugpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43maug_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearn_on_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, LAugPC2):\n\u001b[0;32m     51\u001b[0m     train_laugpc2(\n\u001b[0;32m     52\u001b[0m         model,\n\u001b[0;32m     53\u001b[0m         optimiser,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m         save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     65\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\LAugPC\\train.py:138\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(online_model, optimiser, train_dataset, val_dataset, num_epochs, batch_size, beta, aug_scaler, learn_on_ss, writer, save_dir, save_every)\u001b[0m\n\u001b[0;32m    135\u001b[0m         loss \u001b[38;5;241m=\u001b[39m smooth_l1_loss(pred, target, beta)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Update online model\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimiser)\n\u001b[0;32m    140\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "\n",
    "    if isinstance(model, AugPC):\n",
    "        train_augpc(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=3,\n",
    "            batch_size=128,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "            aug_scaler='none'\n",
    "        )\n",
    "\n",
    "    if isinstance(model, LAugPC):\n",
    "        train_laugpc(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            beta=0.05,\n",
    "            aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, LAugPC2):\n",
    "        train_laugpc2(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            beta=None,\n",
    "            aug_scaler='none',\n",
    "            use_target_model=False,\n",
    "            weights=None,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, BYOL):\n",
    "        train_byol(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            tau_0=0.996,\n",
    "            tau_e=0.999,\n",
    "            tau_T=100,\n",
    "            normalise=True,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, SimSiam):\n",
    "        train_simsiam(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, SimCLR):\n",
    "        train_simclr(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            temperature=1.0,\n",
    "            augmentation=augmentation,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.9564999341964722\n"
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b570ca3c624f7bbec476d377856f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    # img_pred = model.predict(img, action)\n",
    "    img_pred = model.predict(img.flatten(1), action).view(img.shape)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
