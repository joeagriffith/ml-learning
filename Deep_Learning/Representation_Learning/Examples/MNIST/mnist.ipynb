{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = LAugPC\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-5'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{model_name}.pth'\n",
    "# log_dir = None\n",
    "# save_dir = None\n",
    "model = Model(in_features=1, num_actions=5, backbone=backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found, training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [51/500]:  38%|███▊      | 71/188 [00:00<00:00, 142.95it/s, train_loss=0.00234, val_loss=0.00409] "
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "    # train_augpc(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=128,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    #     aug_scaler='none'\n",
    "    # )\n",
    "\n",
    "    train_laugpc(\n",
    "        model,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        num_epochs=500,\n",
    "        batch_size=256,\n",
    "        lr=0.0001,\n",
    "        wd=1.5e-6,\n",
    "        beta=0.05,\n",
    "        tau_0=0.95,\n",
    "        tau_e=0.999,\n",
    "        tau_T=100,\n",
    "        writer=writer,\n",
    "        save_dir=save_dir,\n",
    "        save_every=5,\n",
    "        aug_scaler='none',\n",
    "        learn_on_ss=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    # train_byol(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=1.5e-6,\n",
    "    #     augmentation=augmentation,\n",
    "    #     beta=0.996,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    # train_simclr(\n",
    "    #     model,\n",
    "    #     train_set,\n",
    "    #     val_set,\n",
    "    #     num_epochs=500,\n",
    "    #     batch_size=256,\n",
    "    #     lr=3e-4,\n",
    "    #     wd=0.0,\n",
    "    #     temperature=1.0,\n",
    "    #     augmentation=augmentation,\n",
    "    #     writer=writer,\n",
    "    #     save_dir=save_dir,\n",
    "    #     save_every=5,\n",
    "    # )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# collect 100 of each target index from train_set.targets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmnist_linear_1k_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Examples\\MNIST\\mnist_linear_1k.py:85\u001b[0m, in \u001b[0;36mmnist_linear_1k_eval\u001b[1;34m(model, writer)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m     84\u001b[0m     z \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m---> 85\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_pred, y)\n\u001b[0;32m     87\u001b[0m epoch_val_loss[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f950b8bf675c458bb9a3d5687495816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    img_pred = model.predict(img, action)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
