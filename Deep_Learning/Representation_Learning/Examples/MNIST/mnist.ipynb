{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2.functional as F_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Utils.dataset import PreloadedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.train import train as train_augpc\n",
    "from Deep_Learning.Representation_Learning.Methods.AugPC.model import AugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.train import train as train_laugpc\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC.model import LAugPC\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.train import train as train_byol\n",
    "from Deep_Learning.Representation_Learning.Methods.BYOL.model import BYOL\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.train import train as train_simclr\n",
    "from Deep_Learning.Representation_Learning.Methods.SimCLR.model import SimCLR\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.train import train as train_simsiam\n",
    "from Deep_Learning.Representation_Learning.Methods.SimSiam.model import SimSiam\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC2.train import train as train_laugpc2\n",
    "from Deep_Learning.Representation_Learning.Methods.LAugPC2.model import LAugPC2\n",
    "\n",
    "from Deep_Learning.Representation_Learning.Examples.MNIST.mnist_linear_1k import mnist_linear_1k_eval\n",
    "from Deep_Learning.Representation_Learning.Utils.functional import get_optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root='../Datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "t_dataset = datasets.MNIST(root='../Datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "n_val = int(len(dataset) * VAL_RATIO)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform()\n",
    "])\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomCrop(20),\n",
    "    # transforms.Resize(28, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    # transforms.RandomAffine(degrees=180, translate=(0.28, 0.28), scale=(0.75, 1.25), shear=25),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.75, 1.25), shear=25),\n",
    "    # transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "train_set = PreloadedDataset.from_dataset(train_set, train_transform, device)\n",
    "val_set = PreloadedDataset.from_dataset(val_set, val_transform, device)\n",
    "test_set = PreloadedDataset.from_dataset(t_dataset, val_transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAACMCAYAAAA9QmNpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqYUlEQVR4nO3deZzN1f/A8fcYO8kWZYlE9iQMiRJlSGUqiYhvkVL6SSIVkaSFQdJC9ihJKKWdFmVJluwNRagYZMtuPr8/PDqdc2bunTsz93Pnfj7zej4ePR7vM+/7ufeY99zt9DnvT4zjOI4AAAAAAAAAYZYruycAAAAAAAAAf2LhCQAAAAAAAK5g4QkAAAAAAACuYOEJAAAAAAAArmDhCQAAAAAAAK5g4QkAAAAAAACuYOEJAAAAAAAArmDhCQAAAAAAAK5g4QkAAAAAAACu8OzC0/bt2yUmJkZGjhwZtvv8+uuvJSYmRr7++uuw3Scyhrr6F7X1J+rqT9TVn6irP1FX/6K2/kRd/Ym6BhfRhaepU6dKTEyMrFy5MpIPGzFbtmyRPn36SOPGjSV//vwSExMj27dvz+5puc7vdRUR2b17t7Rv316KFi0qRYoUkbZt28qvv/6a3dNyXU6o7ZdffinXXXedlCxZUooWLSpxcXHy1ltvZfe0XOX3us6dO1fuvPNOqVSpkhQsWFCqVq0qffv2lYMHD2b31Fzl97rOmzdP4uPjpUyZMpIvXz4pV66ctGvXTtavX5/dU3OV3+sqkjPfY/1e14oVK0pMTEya/1WpUiW7p+cqv9eW7zv+rCufnfxZV5Ho+a6TO+KP6GNLly6VsWPHSo0aNaR69eqyZs2a7J4SwuDo0aNy3XXXyaFDh+TJJ5+UPHnyyOjRo+Xaa6+VNWvWSIkSJbJ7isikDz/8UBISEuSqq66SIUOGSExMjMyePVu6dOki+/btkz59+mT3FJEJPXr0kDJlykjnzp3l4osvlnXr1sm4ceNk4cKFsmrVKilQoEB2TxGZsG7dOilWrJj07t1bSpYsKX/99ZdMnjxZ4uLiZOnSpVKnTp3sniIygfdYfxozZowcPXrU+NmOHTtk4MCB0rJly2yaFcKB7zv+xGcnf4qm7zosPIXRLbfcIgcPHpTzzjtPRo4cyQuxT7z22muSlJQkK1askAYNGoiISOvWraVWrVqSmJgow4cPz+YZIrPGjRsnF110kSxatEjy5csnIiL333+/VKtWTaZOncrCk0fNmTNHmjVrZvysXr160rVrV5k5c6Z07949eyaGLHn66adT/ax79+5Srlw5ef311+WNN97Ihlkhq3iP9aeEhIRUPxs2bJiIiHTq1CnCs0E48X3Hn/js5E/R9F0n6no8nTp1Sp5++mmpV6+enH/++VKoUCFp2rSpLF68OOAxo0ePlgoVKkiBAgXk2muvTfO0+82bN0u7du2kePHikj9/fqlfv758+OGH6c7n2LFjsnnzZtm3b1+6ty1evLicd9556d4uJ/JyXefMmSMNGjRQH4hFRKpVqyYtWrSQ2bNnp3u833m5tocPH5ZixYqpF2IRkdy5c0vJkiVz/P/Z8XJd7Q9OIiK33nqriIhs2rQp3eP9zMt1TUupUqWkYMGCvt8KkB4v15X32MC8XNe0vP3223LJJZdI48aNM3W8n3i5tnzfCczLdeWzU2Berms0fdeJuoWnw4cPy8SJE6VZs2by4osvypAhQyQ5OVni4+PTXFGfPn26jB07Vh566CF54oknZP369dK8eXPZs2ePus2GDRukUaNGsmnTJhkwYIAkJiZKoUKFJCEhQebNmxd0PitWrJDq1avLuHHjwv1PzVG8WteUlBT5+eefpX79+qlycXFxsm3bNjly5EhovwSf8mptRc69yW7YsEEGDRokW7dulW3btsmzzz4rK1eulP79+2f4d+EnXq5rWv766y8RESlZsmSmjvcLP9T14MGDkpycLOvWrZPu3bvL4cOHpUWLFiEf70derSvvscF5ta5pWb16tWzatEnuuuuuDB/rR36qLf7jt7ry2ekcL9c1qr7rOBE0ZcoUR0ScH3/8MeBtzpw545w8edL42d9//+2ULl3auffee9XPfvvtN0dEnAIFCji7du1SP1++fLkjIk6fPn3Uz1q0aOHUrl3bOXHihPpZSkqK07hxY6dKlSrqZ4sXL3ZExFm8eHGqnw0ePDhD/9YRI0Y4IuL89ttvGTrOi/xc1+TkZEdEnKFDh6bKvfrqq46IOJs3bw56H17m59o6juMcPXrUad++vRMTE+OIiCMiTsGCBZ358+ene6yX+b2uaenWrZsTGxvr/PLLL5k63gtySl2rVq2qnq+FCxd2Bg4c6Jw9ezbk473Gz3XNye+xfq5rWvr27euIiLNx48YMH+s1Oam2fN8x+aWu/+Kz0zlerms0fdeJujOeYmNjJW/evCJy7v+EHThwQM6cOSP169eXVatWpbp9QkKClC1bVo3j4uKkYcOGsnDhQhEROXDggCxatEjat28vR44ckX379sm+fftk//79Eh8fL0lJSbJ79+6A82nWrJk4jiNDhgwJ7z80h/FqXY8fPy4iYpye+K/8+fMbt8mpvFpbkXN1veyyy6Rdu3byzjvvyIwZM6R+/frSuXNnWbZsWQZ/E/7i5bra3n77bZk0aZL07dvX91dTSo8f6jplyhT59NNP5bXXXpPq1avL8ePH5ezZsyEf70derSvvscF5ta62lJQUmTVrltStW1eqV6+eoWP9yi+1hclPdeWz03+8XNdo+q4Tlc3Fp02bJomJibJ582Y5ffq0+vkll1yS6rZpPREuu+wy1Rdg69at4jiODBo0SAYNGpTm4+3du9f444A7vFjXf/e+njx5MlXuxIkTxm1yMi/WVkSkV69esmzZMlm1apXkynVuHb59+/ZSs2ZN6d27tyxfvjzLj+FlXq2r7rvvvpNu3bpJfHy8PPfcc2G9b6/yel2vuuoqFXfo0EF9kR05cmTYHsOLvFhX3mPT58W62r755hvZvXs3F+yw+KG2SM0PdeWzU2perWs0fdeJuoWnGTNmyP/+9z9JSEiQfv36SalSpSQ2Nlaef/552bZtW4bvLyUlRUREHnvsMYmPj0/zNpUrV87SnJE+r9a1ePHiki9fPvnzzz9T5f79WZkyZbL8OF7m1dqeOnVKJk2aJP3791cvxCIiefLkkdatW8u4cePk1KlT6v9w5DReratu7dq1csstt0itWrVkzpw5kjt31L3lRZwf6qorVqyYNG/eXGbOnJmjF568WlfeY4Pzal1tM2fOlFy5cknHjh3Dft9e5ZfawuSHuvLZKTWv1jXavutE3V/SnDlzpFKlSjJ37lyJiYlRPx88eHCat09KSkr1s19++UUqVqwoIiKVKlUSkXO/4Ouvvz78E0ZIvFrXXLlySe3atWXlypWpcsuXL5dKlSrl+Ct7eLW2+/fvlzNnzqS5Ref06dOSkpKSo7fveLWu/9q2bZu0atVKSpUqJQsXLpTChQu7/phe4PW6puX48eNy6NChbHnsaOHVuvIeG5xX66o7efKkvP/++9KsWbMcvYho80NtkZrX68pnp7R5ta7R9l0nKns8iYg4jqN+tnz5clm6dGmat58/f76xB3LFihWyfPlyad26tYicu9Rys2bNZPz48Wn+H7Xk5OSg88nqpWNxjpfr2q5dO/nxxx+ND8ZbtmyRRYsWyR133JHu8X7n1dqWKlVKihYtKvPmzZNTp06pnx89elQWLFgg1apVy9FbPLxaV5FzV2Fp2bKl5MqVSz777DO54IIL0j0mp/ByXffu3ZvqZ9u3b5evvvoqzaui5SRerivvsYF5ua7/WrhwoRw8eFA6deoU8jE5gR9qi9S8XFc+OwXm1bpG23edbDnjafLkyfLpp5+m+nnv3r3lpptukrlz58qtt94qbdq0kd9++03eeOMNqVGjhhw9ejTVMZUrV5YmTZpIz5495eTJkzJmzBgpUaKEcXnAV199VZo0aSK1a9eW++67TypVqiR79uyRpUuXyq5du2Tt2rUB57pixQq57rrrZPDgwek28Dp06JC88sorIiLy/fffi4jIuHHjpGjRolK0aFHp1atXKL8ez/JrXR988EF58803pU2bNvLYY49Jnjx5ZNSoUVK6dGnp27dv6L8gD/NjbWNjY+Wxxx6TgQMHSqNGjaRLly5y9uxZmTRpkuzatUtmzJiRsV+SB/mxriIirVq1kl9//VX69+8vS5YskSVLlqhc6dKl5YYbbgjht+Ndfq1r7dq1pUWLFnLFFVdIsWLFJCkpSSZNmiSnT5+WF154IfRfkEf5ta45/T3Wr3X918yZMyVfvnxy++23h3R7P/Frbfm+48+68tnJf3WNuu86kbh03r/+vVxhoP927tzppKSkOMOHD3cqVKjg5MuXz6lbt67z0UcfOV27dnUqVKig7uvfyxWOGDHCSUxMdMqXL+/ky5fPadq0qbN27dpUj71t2zanS5cuzoUXXujkyZPHKVu2rHPTTTc5c+bMUbfJ6uUK/51TWv/pc/cbv9fVcRxn586dTrt27ZwiRYo4hQsXdm666SYnKSkps78yz8gJtZ05c6YTFxfnFC1a1ClQoIDTsGFD4zH8yO91DfZvu/baa7Pwm4tufq/r4MGDnfr16zvFihVzcufO7ZQpU8bp0KGD8/PPP2fl1xb1/F5Xx8mZ77E5oa6HDh1y8ufP79x2222Z/TV5kt9ry/cdf9aVz07+rKvjRM93nRjH0c4ZAwAAAAAAAMIk6no8AQAAAAAAwB9YeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK3KHesOYmBg354EMcBwnbPdFXaMHdfWncNZVhNpGE56z/kRd/Ym6+hPvsf7Fc9afqKs/hVJXzngCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK1h4AgAAAAAAgCtyZ/cE4A+5c//3pxQXF2fkfvjhh0hPx5Ar13/rq7169TJygwcPVnHx4sWNXExMjLsTAwAAAKJE+fLlVbx9+3YjFxsbG+HZIJgiRYoY47vuukvFQ4cONXIVK1ZU8bFjx1ydFxAIZzwBAAAAAADAFSw8AQAAAAAAwBVstUNYnDlzRsXZvbWuWrVqxnj48OEqbtu2bcDjUlJSjHHZsmVVvHv37jDNDm64/fbbjXG7du0C3rZOnToBc2vXrlXx1q1bjdygQYMyOTuIiDz77LMqvummm4LeduXKlSoeM2aMkduwYUNY54XU7NP3hwwZouKiRYsauU6dOqk4T548Rs5+DlWpUkXFmzZtMnLLli1T8dSpU43ct99+m+6cAQBZp39GchzHyCUmJqq4b9++EZsTznnttdeM8f333x/ysY0bN1bxl19+GbY5IevKlCljjHv06GGM9e8fXt/uyhlPAAAAAAAAcAULTwAAAAAAAHAFC08AAAAAAABwRYxjb+ANdEMfXlr+rbfeMsbx8fEqbt26tZH76aefIjKnUIRYspB4ta56D5Ju3boZuWuuucYY33LLLSo+deqUkdPHhQsXDvh4uXOb7dDCWQM37tOrdc2ILl26qHjy5MlGTt8DbfeZ+eOPP1ScL18+I9ewYcOAj6f3pHnuueeM3MyZMwMeF+6/Fa/WtkOHDipOSEgwcsF6ciUnJxvjFi1aqHjjxo3hmVwm+fU5a/dBW7FihYrtPk5///23io8ePRryY9j/3nLlyql43759Ru7KK69U8a5du0J+jMzya11zOupqqlq1qjGuUKGCMW7atKmKL7/8ciO3fft2FR8+fNjI6X0+P/nkk6xOM128x2aN3XNR/zxVvHhxI/foo4+qeOzYse5OTHjOiojUqFFDxV9//bWRK1GiRMj38/nnn6u4T58+Rm7z5s2Zm1wmebmu9vOlf//+KrZfQ3V2j+E1a9aouFGjRkZuyZIlAe/HrvmhQ4cC3jbSQqkrZzwBAAAAAADAFSw8AQAAAAAAwBU5eqvdjz/+aIz10/knTJhg5Hr27BmROYXCy6coZoS+paNXr15GTj9NtGzZsiHfp30/ixYtUvFnn31m5MqXL6/iN99808g98MADIT9mqHJKXTPrggsuMMZJSUkq3rFjh5F76aWXVDx//nwj988//6jY3kJZunRpFV966aVG7ptvvlHxyy+/bOQeeeSRgPNmG0D6unbtaoxfeOEFFV944YVGbs+ePSq2t9X+8ssvLswusJzynNVPAy9UqJCR03/nO3fuDPk+7UsC9+vXT8XDhw83cvploJctWxbyY2RWTqmrvi3Afg31Iy/X1d4WfvLkyYC3rVy5sorvu+++gLezWw/ox4mI3HnnnSpetWqVkatUqZKKixYtauTGjBmjYn1rllt4j824ggULqlj/bCMiUrdu3YDH2Z+Z3Obl52y4XHHFFSqeN2+ekbv44oszdZ/ff/+9MW7VqpWKjx07lqn7zAgv17VMmTLGWG/FY39PCUb/LKvXWCT1d4z27durWN/mLJL6dTs7sdUOAAAAAAAA2YaFJwAAAAAAALiChScAAAAAAAC4IrKbdaOMvS80V67/1uHsyznDfXnz5jXGzzzzjIr1y1VmhX1J4Ndff13FAwYMMHLTp09Xsd0nQe93YPcDgzv0y7aLiGzatEnFdm+ZmTNnhnSfZ86cMca7d+9W8fHjx42cfqn4xx9/PKT7R9qC9XQSSd3XCdnLjb5KF110kTG+9957VWz36lq+fHnYHz+nsPsa6vR+LXpfHhGRBQsWqPjmm28O+7yQWuHChY2x3nPQro/eE+3+++83cvpluytWrBjw8b744gtjHOx199VXXzXGQ4cOVXGxYsUCHofo1KZNGxUH6+n0wQcfRGI6CGLNmjUqrlOnjpHr2LFjwOP03ogiIp06dVLx1VdfbeRGjRql4ocfftjInT59OuS5+pXex+7jjz82chnp66T76quvVNykSRMjd/311wc8bty4cZl6vGjBGU8AAAAAAABwBQtPAAAAAAAAcEWO3mpnX/YvJSVFxfYlK+EOfXudvrVOJPj2On1LVEYu/ZmYmBgwN2vWLGOsn3qqn/IuIjJlypSQHxPhYW+LW716tYr/+eefsDyGfsnq999/38gNHDhQxcEuZY30TZs2LWh+8uTJKrZfp++++24V65eDF0m9RQvRRb/0s326evny5VU8bNgwIxfuy6X72aJFi4zx3LlzVWxfoln/vdrvv/q2r5IlS4b8+L179zbGgwYNCvnYnEj/PduXONe3vtnbOfStUhkxf/58FZcoUcLILV682Bjrdbcf394qi+hWr149Yzx+/HgV221Hdu3apeLbb7/d3YkhQw4fPmyM9Tra7Fzz5s1VXKZMGSOntxP55ptvjNw777yT4Xn6zcGDB1Wsb5ETMVu4ZOSzSqFChVRsrznYW6t1I0eODJgLdly04IwnAAAAAAAAuIKFJwAAAAAAALiChScAAAAAAAC4Ikf1eLL7gej9JkRE9u/fr+JVq1ZFZE45TdmyZY2xfqnnYD2dfvjhB2Pcrl07Fe/Zsycsc/vjjz+Msd3XCdFlxYoVKh4+fLiRe+qpp1R86tSpkO/z0UcfVXHNmjWNXHx8fEaniBDZPZ/0Hk8PPPCAkbMvAY7oZb/eb926VcVPP/20kXvhhRciMievqFy5sjFu1aqVijPSx+Gaa64J6XZ6ryHbX3/9ZYzXr19vjGvVqqXi559/3sjpn7t27NgR0lxykhtuuEHFefLkMXL630D79u0D3ofdb0l/r5o6daqR03ur2XW16X0NN23aZORiY2NVPHr0aCOnv48iOuiftUVEihQpouIFCxYYuYSEhEhMCRG2ZMkSFduvJ507d1YxPZ2Cs3ui5cr13zk8eq9o28yZM41xly5dVGz3YCtXrpwx1vuu2WsZXsMZTwAAAAAAAHAFC08AAAAAAABwhe+22tmnqy1fvlzF9ulxDRo0MMZsr3OHfvr4K6+8YuTatm2rYns73R133KHi9E4JD4fZs2cb44cfftj1x0TmXXnllSpeuHChkQt1e13r1q2N8SOPPKJi+/LR+tYCZM26deuC5vVL0tqXc544caIrc0LmlC9fXsVffvmlkbNPFx88eLCK2VoXnL4tUSQ8l0nWX99ERMaOHati/f1WROSee+5RcbVq1Yxc7dq1Az7GE088EXC8ceNGIzd06FAVz5kzJ+B9+pn+3mX/fo4cOaLiSZMmhXyfw4YNy/rEROS2225Tsb1tNiOXDUdkVKxYUcXbtm0zcna9PvroIxV369bN1XkhexQsWNAY61uikXn2++jZs2cD3lZvA2K3Fzhw4ICK9a2vIqmfr6tXr87oNKMWZzwBAAAAAADAFSw8AQAAAAAAwBUsPAEAAAAAAMAVvuvxZNP7Ou3cudPI/f7775GeTo7Qp08fYzxy5MiQjktMTDTGkejrpNuyZUtEHw9ZM2rUKBV/9913Ru6xxx5T8aWXXmrk9F4mJUqUCHj/AwYMMMYnTpzI1DxxTv369VWs96IQESlQoIAxtvs6IXvp/Qf03j8iIt27d1dx6dKljZx9W7uPHsJD/5yzYcMGI6e/Nto9Fu2xLljPpRo1ahjj66+/XsX667I9t5o1axq5WbNmpXk7EbPvjN6PRkRk3759AefmNSdPnlRxtH0G0Xs8BTN37lyXZ4K02J9fPvnkk5CPff7551W8f//+sM0J0UP/zCUikj9//myaib/Y33FHjx6tYrs305NPPqli/XuJiNm3qVGjRkbOvh/9M7P9vPfa85czngAAAAAAAOAKFp4AAAAAAADgCt9vtdNPV0tOTjZyfjpdO7vpWzHs0xCD0S8fnN31+Omnn4yxvrWKU1Sjz/bt21W8ePFiIzdixIhM3af+erF3796AOaTP3j73zDPPqLhw4cJGLikpyRivXbtWxXqd4Z7ixYuruFevXkauZ8+eKra30+nsLZLz5s0L0+xgb0XTBXttuv/++1X80ksvGbkdO3Zkai76+7Y9Hjt2rJF78803VWxvvQxm4sSJAXOTJ09WcY8ePUK+TwSnb0MXEWnRokXA2+qf19atW+fanBBYwYIFjXGVKlUC3nbNmjXGmFYj3pEvXz4V161b18gNHTo04HFNmjQJeD82vbXE3XffbeT07WL2c/3s2bMB79Ov7Pe4ypUrq1h/vxURiY2NVXHevHmNXMOGDUN+zCNHjqjYa1vrbJzxBAAAAAAAAFew8AQAAAAAAABXsPAEAAAAAAAAV/iux9M111xjjIP1RUD46Jc+Llu2bMDbffvtt8a4ffv2KrZ7cAGhsnsdjB8/XsX2nmudvT9dv9zplClTwjQ7fylVqpSK7V5NI0eOVPEtt9wS8D7snk4tW7Y0xvR1irxKlSqpeMiQIZm6j+nTpxvjZcuWGeN+/fqp2O45AlPr1q2N8Ysvvqji/v37BzyuRo0aAXMzZswwxk2bNs3k7DLHvmT0Rx99pOKSJUsauWCf3SI9b7/S+7qJiLzwwgvGuFChQgGP1S8hfujQofBODAGVKVNGxR9++KGR058zuXKZ5xVMmjTJGP/5558uzA7hYPcCql+/vort71DhUqtWrTRjEZH4+HgVz5kzx8g999xzKt68ebORO3XqVDinGLX+7//+L81YxOwH9eCDDwa8D7tPo/3+d9VVV6nYfv+3ezdGO854AgAAAAAAgCtYeAIAAAAAAIArWHgCAAAAAACAK2Ice2NhoBtGca+kRx55RMV6jxERkdWrV6u4QYMGkZqSq0IsWUjCVVd9v7+9x/Wvv/5ScZ06dYzcvn37wvL44dCzZ09jPG7cOBXbvTHuueceFaekpITl8aOxrl5RoUIFY3zbbbepeNSoUQGPs3tTXHTRRSo+fvx4WOYWzrqKZKy25513nor1PeIiInfffXfA4/QeS3FxcUbu4osvVnGJEiWMnN0zRKf3mLB7CHm134SfnrN6Xe1eBLNmzVKx/fewY8cOFd93331GLiEhwRjr/SmaN2+e6bm6LRrrumXLFhVfeumlYXn8vXv3qlh/7YuUadOmqVj/+xMRadasmYrt91i9Pna/uOrVqwd8vGisa6Tpr9lvv/22kbvhhhsCHqd/jhMx+8AcOHAgTLPLnOx8j420hQsXqtjujaibOHGiMX7ooYeMsd3fMlrllOdsbGysiocNG2bkgvX0y6zTp08HfHz79TZ37tDaQc+ePdsYr1+/XsV2/7gzZ86EdJ+hiOa66p5//nljHKyudo82vX9Wq1atjJz+GSy7hfJ85YwnAAAAAAAAuIKFJwAAAAAAALgitPPnotx3332nYq+ccud19mV2O3bsGPC2r732moqjaWudiMiqVatUbG8D1C9R+cQTT0RsTsg4e6vdiBEjVGxvA9AvQdy5c2cjp58qfPPNN4dzihHxwAMPGOPevXuruEqVKiHfj/46Gq5T3bt3765ie4vAwYMHjbG+PVJ/jto2bNgQlrlB5Pfff1fxgAEDAt5uzZo1AXOfffaZMbZPvbe33iF0VatWVbG+7U7E3HoX7DNQsOdysK0P9uW1zz//fGO8bt06FR87dizg/di6du0aMDdmzBgVP/zwwyHf56ZNm1QcbNtdTtWoUSMVB9taJ2JugS5btqxrc0Lo7OdeIBs3bjTGXtlal1PpbWLsdiXh8uuvv6rY3q515ZVXqth+vde3hOXNmzfg/bdv394Y69v5rrvuuoxN1oc++OADY9yvX7+At7W3O+rfnY8ePRreiUUYZzwBAAAAAADAFSw8AQAAAAAAwBUsPAEAAAAAAMAVMU6IDTyiqXdSvXr1jPHy5ctVvHr1aiPXoEGDiMwpkqLh8qL6pTdFRCZPnqxiu2/OypUrVdywYcNMPV642Jeh/vHHH1Vs753Xb6tfXt4t0VBXL9m5c6eKy5UrZ+RefvllFT/yyCNGTu81NGHCBCN31113qfjdd981cpmtj9uXeg7WuyGzj60/xnvvvWfkPv30UxUnJycHvA+7D5zeU6BLly4hz+Wff/4xxnrPuOzuvcZz1qT3VhMR6du3rzHu0KGDiu3+T9Ek2utqP3/0XhH2ZbJ1efLkMcah9kDS+3+lRe+x16ZNGyP3xx9/qPjnn38O6fFsU6ZMMcb6v9+uVbBLf0d7Xd1gv//p/fP0Hk4iIm3btjXG+me3aOb2e2x22rt3rzEuXry4iu3eiCVLlozElCLKr8/ZV155xRjr/Tlz5cr8OSFbt25V8Y033mjktm3blqn7LFy4sIovv/xyI6d/3ytWrJiRu+OOO1Ss92IWEWnWrFmm5pKWaKprMHp/PRGRJUuWBLyt/W/SnwdXX321kdPXQLJbKM9XzngCAAAAAACAK1h4AgAAAAAAgCsCn5PsIV45zc5P9Et/ipinW+7fv9/IdezYMSJzCkTfMrdw4UIjp2+vmzp1qpFLb3sBIqtx48bGWL+881dffWXknnrqqYD3M3HiRBXrpwKLiLzzzjsqti8bv3nz5pDnml3CveUgo/TtdfblejOyvQ7eZG9zOnHihDHWtwEg86ZPnx50rLvgggtU3Lp1ayOnX0J7zJgxRm7BggUqrlGjhpGzt4Lo233tbXHBHD9+XMUbNmwwcuPHjw/5fnT658Hsfj3MLvqWjmeffTbg7b799ltj7JWtdX63Y8cOFetb60TM7XUtW7aM1JSQCWPHjjXGPXv2VHFWttPp5s2bZ4wHDBig4sxurbMdPXpUxT/88IOR08d58+Y1cvrn8AMHDhi5v//+Oyxz8xJ7K2Jm2Z+lo2mrXSg44wkAAAAAAACuYOEJAAAAAAAArmDhCQAAAAAAAK6IcULcBB9NfZTq1atnjPX9jatXrzZyDRo0iMicIikaLi9aqVIlY5yUlBTwtnrfgIYNG2bq8bJCv3xw7969A95O7wUlIrJ9+3a3ppSmaKhrNClQoIAxPnTokDHWLw2u9zEREdm3b1/A+9WPs/erx8bGqjguLs7InTlzJp0Zp83tSz3r/VWCSU5ONsZff/21iocOHRrwuI0bN4Y+uSAqVqyo4kGDBhm5du3aBTyuSJEixjglJUXFei8MEbOfyZw5c4zckSNHQp5rqHjOmpdgnzVrlpHT+6mJiDz88MMRmVNWUVezH0nVqlWD3lava0Z+d+Hox2T/fvXXcJtf61qqVCljrNcjWL/D5s2bG2P9PcFL3H6PdVufPn2Msd1DVbdr1y4VV6hQwbU5RQsvP2ftueufXTJi/fr1Kr7vvvuMnN2XLbOPEWlermu4fP/99ypO77txsPfKpUuXqrhp06Zhml3mhFJXzngCAAAAAACAK1h4AgAAAAAAgCtyp3+T6KBvr1uxYoWR008JnzBhQsTmlJPt2bPHGOuXQq5Zs6aRq1u3roqfeeYZIzd48OAsz8Xe9mdfXrR69eoBj9Xn9vvvv2d5LgifEiVKGGN9i5yIuZXKvlRrMLfffruK69evb+Tmz5+v4sxurXObfgl0EZGuXbuGdJy99cy+1L3b9K2r3bp1M3KJiYnGWP83xsfHGzn79UWnbzGxt5HcfffdIc8Vgelb60RE3nnnnYC3tbeywjtef/31kG+rP+/WrVtn5G699VYVL1iwwMidPn1axXXq1DFy+hYSu72Czj61v0aNGioO1zbhaKRvL7df25s0aRLwuJ07d6o4WIsEuOuKK65Qsb31XP+b3r9/v5Hr0aOHq/NC+NivP1WqVFGxvSX42LFjKl61apWR0z+78D3FP/Ttc+ltF8yV67/zhLyynTIQzngCAAAAAACAK1h4AgAAAAAAgCtYeAIAAAAAAIArPNPjSRfuS6ci4/755x9jfMMNN6j4iy++MHJ6T5YnnnjCyN12220qfu6554ycfmlu+7Kxet+IBx980MhdeumlxljfZz1ixAgjp1+m1Ov7Zv3G7u1j0/9egtWuZMmSxvjxxx9Xsb1fPli/mmhh7/+3x15k90LQxzNmzIj0dHI8vZ+AiHm59rFjxxq5vHnzqvjFF180cl54PiHr7H4luooVK6rY7rv22WefpRnbLrroImOcnJysYr2nk4i/+zrp9M85ofZ0EhFp06aNinfv3h3+iSFNpUuXNsZ6v7Pzzz/fyOnfcT7//HMjF+x5guhi96Ps0KGDivPnz2/k9Ne0jz/+2N2JISqsWbNGxXFxcUFvq3/H8foaCGc8AQAAAAAAwBUsPAEAAAAAAMAVMU6I52yld6k/t3Xu3FnF06ZNM3L6Vqvx48dHbE7ZJZyn2blRV/uUYn3rXbBLodvbpY4fP65i+1R++zRV3ebNm43xM888o+LZs2cHPC67RXtdI61bt27GeOLEicY4T548Kj5z5oyR009d//TTT41co0aNVNyqVSsj58Zp7OE+LdYPtfWLaHzOjh49WsX6NjgRc3vH9u3bjZy+dadfv35G7pprrlGx/Vzr3r27iqdPn57xCUehaKwrss7Ldb3nnnuMsd42oHjx4gGPGzp0qDEeMmRIWOcVDbzwHlu+fHlj/NtvvwV8PL2Vhf7aK2Juz8kJvPycRWDU1aR/LxERWbJkScDb2v9e/Xe5dOlSI9e0adMwzC50odSVM54AAAAAAADgChaeAAAAAAAA4AoWngAAAAAAAOCK3Nk9gVBt2rRJxT179jRyEyZMiPR0EMSePXuM8fXXX6/ihx56yMjdcccdKq5ataqRK1SokIq3bt1q5PQ+Be+9956Rs3uQwJuSkpKMsV3Xm2++WcU///xzwGOXLVtm5EqUKKHiAwcOZHmeQDRZu3atiidPnmzkunTpomJ7L37hwoVVrPcYERF59tlnVTx48OCwzBPQVatWzRjbvRpzOv39TiR4X6fff/9dxX7s6eQ333zzjTHW+3fltJ5OQE5kf9/55JNPjPGNN96oYvuzm97XKdI9nTKDM54AAAAAAADgChaeAAAAAAAA4IoYJ8RrGkb6coWjRo0yxo8++mhEHz+acRlKf6KupjFjxhjj3r17h3ysfgnp7N4a5IVLPSNzvPacTUlJUfGsWbOM3LBhw1S8ceNG1+cSzbxWV6+qUaOGiiPxN+e1us6dO1fFCQkJRm7nzp0qbtmypZHbsmWLq/OKNrzH+pfXnrMIDXUN7rzzzjPG3bp1U3FiYqKR07/v2FvU3333XRdmF1godeWMJwAAAAAAALiChScAAAAAAAC4goUnAAAAAAAAuCJqezz16NHDGE+YMCGijx/N2BvrT9TVn+g/4V88Z/2JuvqTl+v6yy+/GGO9d8egQYMiOpdow3usf3n5OYvAqKs/0eMJAAAAAAAA2YaFJwAAAAAAALgiarfaITBOUfQn6upPbAPwL56z/kRd/Ym6+hPvsf7Fc9afqKs/sdUOAAAAAAAA2YaFJwAAAAAAALiChScAAAAAAAC4IuQeTwAAAAAAAEBGcMYTAAAAAAAAXMHCEwAAAAAAAFzBwhMAAAAAAABcwcITAAAAAAAAXMHCEwAAAAAAAFzBwhMAAAAAAABcwcITAAAAAAAAXMHCEwAAAAAAAFzBwhMAAAAAAABc8f+34JoKh2QL6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: tensor(1., device='cuda:0')\n",
      "Min value: tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show example images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15,5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_set[i]\n",
    "    angle = torch.rand(1).item() * 360 - 180 if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_x = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    translate_y = torch.randint(-8, 9, (1,)).item() if torch.rand(1).item() > 0.75 else 0\n",
    "    scale = torch.rand(1).item() * 0.5 + 0.75 if torch.rand(1).item() > 0.75 else 1.0\n",
    "    shear = torch.rand(1).item() * 50 - 25 if torch.rand(1).item() > 0.75 else 0\n",
    "    img = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max and min values\n",
    "print('Max value:', train_set.transformed_images.max())\n",
    "print('Min value:', train_set.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = BYOL\n",
    "backbone = 'alexnet'\n",
    "model_name = f'{Model.__name__}-{backbone}-test'\n",
    "log_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/logs/{model_name}/'\n",
    "save_dir = f'Deep_Learning/Representation_Learning/Examples/MNIST/out/models/{model_name}.pth'\n",
    "log_dir = None\n",
    "save_dir = None\n",
    "# model = Model(1, 5, backbone=backbone).to(device)\n",
    "model = Model(1, backbone).to(device)\n",
    "\n",
    "\n",
    "optimiser = get_optimiser(\n",
    "    model, \n",
    "    'AdamW', \n",
    "    lr=3e-4, \n",
    "    wd=1.5e-6, \n",
    "    exclude_bias=True, \n",
    "    exclude_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (struct c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 71\u001b[0m\n\u001b[0;32m     54\u001b[0m     train_laugpc2(\n\u001b[0;32m     55\u001b[0m         model,\n\u001b[0;32m     56\u001b[0m         optimiser,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, BYOL):\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mtrain_byol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.996\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau_e\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearn_on_ss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, SimSiam):\n\u001b[0;32m     91\u001b[0m     train_simsiam(\n\u001b[0;32m     92\u001b[0m         model,\n\u001b[0;32m     93\u001b[0m         optimiser,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m         save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    104\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\BYOL\\train.py:74\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(online_model, optimiser, train_dataset, val_dataset, num_epochs, batch_size, augmentation, beta, tau_0, tau_e, tau_T, normalise, learn_on_ss, writer, save_dir, save_every)\u001b[0m\n\u001b[0;32m     72\u001b[0m x1, x2 \u001b[38;5;241m=\u001b[39m augmentation(images), augmentation(images)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 74\u001b[0m     y1, y2 \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m, online_model(x2)\n\u001b[0;32m     75\u001b[0m     z1, z2 \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39mproject(y1), online_model\u001b[38;5;241m.\u001b[39mproject(y2)\n\u001b[0;32m     76\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39mpredict(z1), online_model\u001b[38;5;241m.\u001b[39mpredict(z2)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\ml-learning\\Deep_Learning\\Representation_Learning\\Methods\\BYOL\\model.py:39\u001b[0m, in \u001b[0;36mBYOL.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torchvision\\models\\alexnet.py:48\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (struct c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "to_train = True\n",
    "if save_dir is not None:\n",
    "    try:\n",
    "        sd = torch.load(save_dir)\n",
    "        # change keys \"project\" to \"transition\"\n",
    "        for key in list(sd.keys()):\n",
    "            if 'project' in key:\n",
    "                sd[key.replace('project', 'transition')] = sd.pop(key)\n",
    "        model.load_state_dict(sd)\n",
    "        to_train = False\n",
    "        print('Model loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        print('Model not found, training new model')\n",
    "if to_train:\n",
    "    writer = None\n",
    "    if log_dir is not None:\n",
    "        writer = SummaryWriter(log_dir)\n",
    "\n",
    "    if isinstance(model, AugPC):\n",
    "        train_augpc(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=3,\n",
    "            batch_size=128,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "            aug_scaler='none'\n",
    "        )\n",
    "\n",
    "    if isinstance(model, LAugPC):\n",
    "        train_laugpc(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            beta=0.05,\n",
    "            tau_0=0.925,\n",
    "            tau_e=0.95,\n",
    "            tau_T=100,\n",
    "            aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, LAugPC2):\n",
    "        train_laugpc2(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            beta=None,\n",
    "            normalise=False,\n",
    "            aug_scaler='none',\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, BYOL):\n",
    "        train_byol(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            tau_0=0.996,\n",
    "            tau_e=0.999,\n",
    "            tau_T=100,\n",
    "            normalise=True,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, SimSiam):\n",
    "        train_simsiam(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            augmentation=augmentation,\n",
    "            beta=None,\n",
    "            learn_on_ss=False,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, SimCLR):\n",
    "        train_simclr(\n",
    "            model,\n",
    "            optimiser,\n",
    "            train_set,\n",
    "            val_set,\n",
    "            num_epochs=500,\n",
    "            batch_size=256,\n",
    "            temperature=1.0,\n",
    "            augmentation=augmentation,\n",
    "            writer=writer,\n",
    "            save_dir=save_dir,\n",
    "            save_every=5,\n",
    "        )\n",
    "\n",
    "    print(f'Finished training')\n",
    "    if save_dir is not None:\n",
    "        print('Run cell again to load best (val_acc) model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.9838999509811401\n"
     ]
    }
   ],
   "source": [
    "# collect 100 of each target index from train_set.targets\n",
    "writer = SummaryWriter(log_dir)\n",
    "mnist_linear_1k_eval(model, writer, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b570ca3c624f7bbec476d377856f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='angle', max=180, min=-180), IntSlider(value=0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare(model, img, angle, translate_x, translate_y, scale, shear)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set[4][0].unsqueeze(0)\n",
    "model.eval()\n",
    "\n",
    "def compare(model, img, angle, translate_x, translate_y, scale, shear):\n",
    "    img_aug = F_v2.affine(img, angle=angle, translate=(translate_x, translate_y), scale=scale, shear=shear)\n",
    "    action = torch.tensor([angle/180, translate_x/8, translate_y/8, (scale-1.0)/0.25, shear/25], dtype=torch.float32, device=img.device).unsqueeze(0).repeat(img.shape[0], 1)\n",
    "    # img_pred = model.predict(img, action)\n",
    "    img_pred = model.predict(img.flatten(1), action).view(img.shape)\n",
    "    loss = F.mse_loss(img_aug, img_pred)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(img_aug.squeeze().cpu(), cmap='gray')\n",
    "    axes[1].set_title('Augmented')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(img_pred.squeeze().cpu().detach(), cmap='gray')\n",
    "    axes[2].set_title('Predicted')\n",
    "    axes[2].axis('off')\n",
    "    plt.show()\n",
    "    return loss.item()\n",
    "\n",
    "interact(compare, model=fixed(model), img=fixed(img), angle=(-180, 180), translate_x=(-8, 8), translate_y=(-8, 8), scale=(0.75, 1.25), shear=(-25, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
