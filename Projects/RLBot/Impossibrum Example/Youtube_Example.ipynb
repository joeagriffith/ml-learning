{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777174ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\anaconda3\\envs\\ml-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from rlgym.envs import Match\n",
    "from rlgym.utils.action_parsers import DiscreteAction\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecNormalize, VecCheckNan\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4100ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgym.utils.obs_builders import AdvancedObs\n",
    "from rlgym.utils.state_setters import DefaultState\n",
    "from rlgym.utils.terminal_conditions.common_conditions import TimeoutCondition, NoTouchTimeoutCondition, GoalScoredCondition\n",
    "from rlgym_tools.sb3_utils import SB3MultipleInstanceEnv\n",
    "from rlgym.utils.reward_functions.common_rewards.misc_rewards import EventReward\n",
    "from rlgym.utils.reward_functions.common_rewards.player_ball_rewards import VelocityPlayerToBallReward\n",
    "from rlgym.utils.reward_functions.common_rewards.ball_goal_rewards import VelocityBallToGoalReward\n",
    "from rlgym.utils.reward_functions import CombinedReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86dfff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 8          # Number of ticks to repeat an action\n",
    "half_life_seconds = 5   # Easier to conceptualize, after this many seconds the reward discount is 0.5\n",
    "\n",
    "fps = 240 / frame_skip\n",
    "gamma = np.exp(np.log(0.5) / (fps * half_life_seconds))  # Quick mafs\n",
    "agents_per_match = 2\n",
    "num_instances = 6\n",
    "target_steps = 1_000_000\n",
    "steps = target_steps // (num_instances * agents_per_match) #making sure the experience counts line up properly\n",
    "batch_size = target_steps//10 #getting the batch size down to something more manageable - 100k in this case\n",
    "training_interval = 25_000_000\n",
    "mmr_save_frequency = 50_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0de3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_save(model):\n",
    "    model.save(\"models/exit_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb90ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match():  # Need to use a function so that each instance can call it and produce their own objects\n",
    "    return Match(\n",
    "        team_size=1,\n",
    "        tick_skip=frame_skip,\n",
    "        reward_function=CombinedReward(\n",
    "        (\n",
    "            VelocityPlayerToBallReward(),\n",
    "            VelocityBallToGoalReward(),\n",
    "            EventReward(\n",
    "                team_goal=100.0,\n",
    "                concede=-100.0,\n",
    "                shot=5.0,\n",
    "                save=30.0,\n",
    "                demo=10.0,\n",
    "            ),\n",
    "        ),\n",
    "        (0.1, 1.0, 1.0)),\n",
    "        # self_play=True,  in rlgym 1.2 'self_play' is depreciated. Uncomment line if using an earlier version and comment out spawn_opponents\n",
    "        spawn_opponents=True,\n",
    "        terminal_conditions=[TimeoutCondition(fps * 300), NoTouchTimeoutCondition(fps * 45), GoalScoredCondition()],\n",
    "        obs_builder=AdvancedObs(),  # Not that advanced, good default\n",
    "        state_setter=DefaultState(),  # Resets to kickoff position\n",
    "        action_parser=DiscreteAction()  # Discrete > Continuous don't @ me\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec5d29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SB3MultipleInstanceEnv(get_match, num_instances)# Start 1 instances, waiting 60 seconds between each\n",
    "env = VecCheckNan(env)                                # Optional\n",
    "env = VecMonitor(env)                                 # Recommended, logs mean reward and ep_len to Tensorboard\n",
    "env = VecNormalize(env, norm_obs=False, gamma=gamma)  # Highly recommended, normalizes rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a58323e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous exit save.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = PPO.load(\n",
    "        \"models/exit_save.zip\",\n",
    "        env,\n",
    "        device=\"auto\",\n",
    "        custom_objects={\"n_envs\": env.num_envs}, #automatically adjusts to users changing instance count, may encounter shaping error otherwise\n",
    "        # If you need to adjust parameters mid training, you can use the below example as a guide\n",
    "        #custom_objects={\"n_envs\": env.num_envs, \"n_steps\": steps, \"batch_size\": batch_size, \"n_epochs\": 10, \"learning_rate\": 5e-5}\n",
    "    )\n",
    "    print(\"Loaded previous exit save.\")\n",
    "except:\n",
    "    print(\"No saved model found, creating new model.\")\n",
    "    from torch.nn import Tanh\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=Tanh,\n",
    "        net_arch=[512, 512, dict(pi=[256, 256, 256], vf=[256, 256, 256])],\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        n_epochs=10,                 # PPO calls for multiple epochs\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        learning_rate=5e-5,          # Around this is fairly common for PPO\n",
    "        ent_coef=0.01,               # From PPO Atari\n",
    "        vf_coef=1.,                  # From PPO Atari\n",
    "        gamma=gamma,                 # Gamma as calculated using half-life\n",
    "        verbose=3,                   # Print out all the info as we're going\n",
    "        batch_size=batch_size,             # Batch size as high as possible within reason\n",
    "        n_steps=steps,                # Number of steps to perform before optimizing network\n",
    "        tensorboard_log=\"logs\",  # `tensorboard --logdir out/logs` in terminal to see graphs\n",
    "        device=\"auto\"                # Uses GPU if available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a905ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs\\PPO_0\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.48e+03    |\n",
      "|    ep_rew_mean     | -0.29294434 |\n",
      "| time/              |             |\n",
      "|    fps             | 2060        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 2911        |\n",
      "|    total_timesteps | 6378186     |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.44e+03    |\n",
      "|    ep_rew_mean          | -0.08185673 |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2029        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5912        |\n",
      "|    total_timesteps      | 12378186    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005279891 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.57       |\n",
      "|    explained_variance   | -0.0173     |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 0.0638      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.45e+03     |\n",
      "|    ep_rew_mean          | -0.07132059  |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2034         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8849         |\n",
      "|    total_timesteps      | 18378186     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032238588 |\n",
      "|    clip_fraction        | 0.00534      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.57        |\n",
      "|    explained_variance   | 0.577        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0176      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 0.0607       |\n",
      "------------------------------------------\n",
      "Exiting training\n",
      "Saving model\n",
      "Save complete\n"
     ]
    }
   ],
   "source": [
    "# Save model every so often\n",
    "# Divide by num_envs (number of agents) because callback only increments every time all agents have taken a step\n",
    "# This saves to specified folder with a specified name\n",
    "callback = CheckpointCallback(round(5_000_000 / env.num_envs), save_path=\"models\", name_prefix=\"rl_model\")\n",
    "\n",
    "try:\n",
    "    mmr_model_target_count = model.num_timesteps + mmr_save_frequency\n",
    "    while True:\n",
    "        #may need to reset timesteps when you're running a different number of instances than when you saved the model\n",
    "        model.learn(training_interval, callback=callback, reset_num_timesteps=False) #can ignore callback if training_interval < callback target\n",
    "        model.save(\"models/exit_save\")\n",
    "        if model.num_timesteps >= mmr_model_target_count:\n",
    "            model.save(f\"mmr_models/{model.num_timesteps}\")\n",
    "            mmr_model_target_count += mmr_save_frequency\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting training\")\n",
    "\n",
    "print(\"Saving model\")\n",
    "exit_save(model)\n",
    "print(\"Save complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2134d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "40782504ef2dfc4e8ee044283236f5af461ad0747c261a561b1d18b0ffd306b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
